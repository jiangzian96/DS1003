{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know $g$ is a subgradient of $f_k(x)$, by definition we have $$f_k(z) \\geq f_k(x) + g^T(z-x), \\forall z,$$ and since f_k(x) = f(x), the above equation can be re-written as $$f(z) \\geq f(x)+g^T(z-x), \\forall z$$, and by definition, this means $g$ is a subgradient of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$g =\n",
    "\\begin{cases} \n",
    "0, 1-yw^Tx < 0\\\\\n",
    "-yx, else\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $w$ does define a separating hyperline, then all data points in $\\mathbb{D}$ will be correctly classified. Thus, the perceptron loss $$l(\\tilde{y},y)=\\max\\{0,-\\tilde{y}y\\}=0,\\forall y$$ because $\\tilde{y}y = 1,\\forall y$ (either $1\\times 1 or -1 \\times -1$). Since all single perceptron loss is 0, the average loss will be 0 too on $\\mathbb{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSGD to minimize empirical risk of perceptron is equivalent to perceptron algorithm. This is because of the gradient w.r.t $w$ of the term $max\\{0,1-yw^Tx\\}$,\n",
    "\n",
    "$$\\nabla max\\{0,1-yw^Tx\\} = \n",
    "\\begin{cases}\n",
    "    0,& \\text{if } yw^Tx\\geq 1\\\\\n",
    "    -yx,              & \\text{otherwise.}\n",
    "\\end{cases}$$ As we can see, running gradient descent using the above equation is equivalent to using an if-else condition to update on data points that aren't correctly classified $(yw^Tx < 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in the perceptron algorithm update rule, for each point $(x_i,y_i)$, according to last question, nothing gets updated if $x_i$ is correctly classified or else $w += y_ix_i$, we can see that when the algorithm converges, all the updates were in the form $y_ix_i$. Thus, the final weight vector $w$ will be a linear combination of the input points. More specifically, we can write $w$ as $w = \\sum_{i=0}^{n}\\alpha_i y_ix_i$, where $\\alpha_i=0$ when $x_i$ is correctly classified else $\\alpha_i=1$. Thus, the support vectors are closer to the separating hyperplane than the non support vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Sparse Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"data.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf data/neg/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('review.pkl', 'rb') as f:\n",
    "    review = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since load.py has already shuffled the data, no need to shuffle again here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(review) == 2000\n",
    "review_train = review[:1500]\n",
    "train_labels = [r[-1] for r in review_train]\n",
    "review_train = [r[:-1] for r in review_train]\n",
    "\n",
    "review_val = review[-500:]\n",
    "val_labels = [r[-1] for r in review_val]\n",
    "review_val = [r[:-1] for r in review_val]\n",
    "\n",
    "assert len(review_train) == 1500\n",
    "assert len(train_labels) == 1500\n",
    "assert len(review_val) == 500\n",
    "assert len(val_labels) == 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Harry': 2, 'Potter': 2, 'and': 1, 'II': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def convert_sparse_representation(list_of_words):\n",
    "    return Counter(list_of_words)\n",
    "convert_sparse_representation([\"Harry\", \"Potter\", \"and\", \"Harry\", \"Potter\", \"II\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: SVM with via Pegasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla J_i(w) = \n",
    "\\begin{cases}\n",
    "    \\lambda w,& \\text{if } y_iw^Tx_i> 1\\\\\n",
    "    -y_ix_i + \\lambda w, \\text{if } y_iw^Tx_i< 1\\\\\n",
    "    \\text{undefined}, \\text{if } y_iw^Tx_i = 1\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is true because $g$ is just the real gradient of $\\nabla J_i(w)$ with the point where it is undefined $\\lambda w$. Thus $g$ is continuous and $g \\leq \\nabla J_i(w).$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w = \n",
    "\\begin{cases}\n",
    "    w -\\eta_t(\\lambda w -y_ix_i) = (1-\\eta_t\\lambda)w + \\eta_ty_ix_i,& \\text{if } yw^Tx < 1\\\\\n",
    "    w -\\eta_t\\lambda w = (1-\\eta_t\\lambda)w,              & \\text{otherwise.}\n",
    "\\end{cases}$$ As we can see, this is equivalent to the Pegaso update rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import time\n",
    "def pegasos_dict(X, y, lambda_reg = 0.1, max_epochs = 1000, verbose = True):\n",
    "    epoch = 0\n",
    "    t = 0.\n",
    "    w = collections.defaultdict(float)\n",
    "    times = []\n",
    "    cur_loss = float(\"inf\")\n",
    "    while epoch < max_epochs:\n",
    "        tic = time.perf_counter()\n",
    "        epoch += 1\n",
    "        for i, y_i in enumerate(y):\n",
    "            t += 1\n",
    "            eta = 1.0/(lambda_reg*t)\n",
    "            x_i = convert_sparse_representation(X[i]) # counter \n",
    "            \n",
    "            temp = {k: v*(1 - eta*lambda_reg) for k, v in w.items()}\n",
    "            \n",
    "            w_dot_xi = sum(x_i.get(k, 0) * v for k, v in w.items())\n",
    "            if y_i*w_dot_xi < 1:\n",
    "                for k, v in x_i.items():\n",
    "                    w[k] = temp.get(k,0) + v * eta * y_i\n",
    "            else:\n",
    "                w = temp.copy()\n",
    "        toc = time.perf_counter()\n",
    "        times.append(toc-tic)\n",
    "        prev_loss = cur_loss\n",
    "        cur_loss = svm_loss(X, y, w)\n",
    "        if verbose:\n",
    "            print('Epoch', str(epoch), \"loss:\", cur_loss)\n",
    "        if cur_loss > prev_loss:\n",
    "            return w, sum(times)/len(times)\n",
    "    return w, sum(times)/len(times)\n",
    "\n",
    "def svm_loss(X, y, w, lambda_reg = 0.1):\n",
    "    reg_penalty = 0.\n",
    "    for k,v in w.items():\n",
    "        reg_penalty += v**2\n",
    "    reg_penalty *= lambda_reg/2\n",
    "    \n",
    "    margin_loss = 0\n",
    "    for i, y_i in enumerate(y):\n",
    "        x_i = convert_sparse_representation(X[i]) # counter \n",
    "        w_dot_xi = sum(x_i.get(k, 0) * v for k, v in w.items())\n",
    "        reg_penalty += max(0,1-y_i*w_dot_xi)\n",
    "    reg_penalty /= len(y)\n",
    "    \n",
    "    return margin_loss + reg_penalty    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_{t+1} = (1-\\eta_t\\lambda)w_t + \\eta_ty_ix_i = (1-\\eta_t\\lambda)s_tW_t + \\eta_ty_ix_i = s_{t+1}W_t + \\eta_ty_ix_i = s_{t+1}(W_t + \\frac{1}{s_{t+1}}\\eta_ty_ix_i) = s_{t+1}W_{t+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pegasos_accerlated(X, y, lambda_reg = 0.1, max_epochs = 1000, verbose = True):\n",
    "    epoch = 0\n",
    "    t = 1.\n",
    "    s_t = 1.\n",
    "    W = collections.defaultdict(float)\n",
    "    w = collections.defaultdict(float)\n",
    "    times = []\n",
    "    cur_loss = float(\"inf\")\n",
    "    while epoch < max_epochs:\n",
    "        tic = time.perf_counter()\n",
    "        epoch += 1\n",
    "        for i, y_i in enumerate(y):\n",
    "            t += 1\n",
    "            eta = 1.0/(lambda_reg*t)\n",
    "            s_t *= (1-eta*lambda_reg)\n",
    "            x_i = convert_sparse_representation(X[i]) # counter \n",
    "            w_dot_xi = sum(w.get(k, 0) * v for k, v in x_i.items())\n",
    "            if w_dot_xi*y_i < 1:\n",
    "                for k, v in x_i.items():\n",
    "                    W[k] = W[k] + (1/s_t)*eta*y_i*v\n",
    "            w = {k: s_t*v for k, v in W.items()}\n",
    "        toc = time.perf_counter()\n",
    "        times.append(toc-tic)\n",
    "        prev_loss = cur_loss\n",
    "        cur_loss = svm_loss(X, y, w)\n",
    "        if verbose:\n",
    "            print('Epoch', str(epoch), \"loss:\", cur_loss)\n",
    "        if cur_loss > prev_loss:\n",
    "            return w, sum(times)/len(times)\n",
    "    return w, sum(times)/len(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 1.2173642174031287\n",
      "Epoch 2 loss: 0.7622658345683799\n",
      "Epoch 3 loss: 0.25437708010300636\n",
      "Epoch 4 loss: 0.1920115289110334\n",
      "Epoch 5 loss: 0.5285685053689315\n",
      "Epoch 1 loss: 1.3517782039747592\n",
      "Epoch 2 loss: 0.7627414042407208\n",
      "Epoch 3 loss: 0.2567636132699729\n",
      "Epoch 4 loss: 0.21322456509243493\n",
      "Epoch 5 loss: 0.38204383077619736\n"
     ]
    }
   ],
   "source": [
    "w1,t1 = pegasos_dict(review_train, train_labels, lambda_reg = 0.1, max_epochs = 1000)\n",
    "w2,t2 = pegasos_accerlated(review_train, train_labels, lambda_reg = 0.1, max_epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first implementation takes on average 32.76 seconds per epoch\n",
      "the accelerated implementation takes on average 16.40 seconds per epoch\n"
     ]
    }
   ],
   "source": [
    "print(\"the first implementation takes on average {0:.2f} seconds per epoch\".format(t1))\n",
    "print(\"the accelerated implementation takes on average {0:.2f} seconds per epoch\".format(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0029994545391896295\n",
      "-0.0026663111585121893\n"
     ]
    }
   ],
   "source": [
    "word = \"hi\"\n",
    "print(w1[word])\n",
    "print(w2[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(w, X, y):\n",
    "    error = 0\n",
    "    total = 0\n",
    "    for i, y_i in enumerate(y):\n",
    "        x_i = convert_sparse_representation(X[i])\n",
    "        w_dot_xi = sum(x_i.get(k, 0) * v for k, v in w.items())\n",
    "        if (w_dot_xi > 0 > y_i) or (w_dot_xi < 0 < y_i):\n",
    "            error += 1\n",
    "        total += 1\n",
    "    return error/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  1e-05\n",
      "Testing:  0.0001\n",
      "Testing:  0.001\n",
      "Testing:  0.01\n",
      "Testing:  0.05\n",
      "Testing:  0.1\n",
      "Testing:  0.5\n",
      "Testing:  1\n",
      "Testing:  10\n"
     ]
    }
   ],
   "source": [
    "lambdas = [1e-5,1e-4,1e-3,1e-2,5e-2,1e-1,5e-1,1,10]\n",
    "errors = []\n",
    "for lambda_reg in lambdas:\n",
    "    print(\"Testing: \", lambda_reg)\n",
    "    w,t = pegasos_accerlated(review_train, train_labels, lambda_reg = lambda_reg,\\\n",
    "                             max_epochs = 10, verbose=False)\n",
    "    errors.append(evaluate(w, review_val, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best lambda is 1e-05 with lowest error rate 0.18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"The best lambda is {}\".format(lambdas[np.argmin(errors)]),\\\n",
    "      \"with lowest error rate {0:.2f}\".format(min(errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $V$ be the total vocabulary from both documents without duplicates. Then for any document $x$, for the $k$-th word $w_k$ in vocabulary $V$, if $w_k$ appears in $x$ then $\\phi(x)_k = 1$ else $0$. Thus, both $\\phi(x)$ and $\\phi(z)$ will be vectors of length $\\vert V\\vert$. Then, $k(x,z) = \\phi(x)^T\\phi(z)$ will be the unique number of words that appear in both documents.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $k(x,z) = x^Tz$ be a kernel, $\\frac{1}{\\Vert x\\Vert_2} = f(x), \\frac{1}{\\Vert z\\Vert_2} = f(z)$. Then $$k_1(x,z) = f(x)f(z)k(x,z) = \\frac{1}{\\Vert x\\Vert_2}\\frac{1}{\\Vert z\\Vert_2}x^Tz = (\\frac{x}{\\Vert x\\Vert_2})^T (\\frac{z}{\\Vert z\\Vert_2})$$ is also a kernel. Since $1$ is also a kernel by constant feature mapping $\\phi(x)=1$, $$k_2(x,z) = 1 + k_1(x,z) = 1 + (\\frac{x}{\\Vert x\\Vert_2})^T (\\frac{z}{\\Vert z\\Vert_2}) $$ is also a kernel. Finally, given $k_2(x,z)$ is a kernel, we can apply the product rule twice, thus $$k_3(x,z)=(k_2(x,z))^3 = ( 1 + (\\frac{x}{\\Vert x\\Vert_2})^T (\\frac{z}{\\Vert z\\Vert_2}))^3$$ is a kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to proving $(w^{(t)})^Tx_j = K_{j.}a^{(t)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, $(w^{(t)})^T = \\begin{pmatrix}\n",
    "a_1 & a_2 & ... & a_n\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "\\vec{x_1}\\\\\n",
    "\\vec{x_2}\\\\\n",
    "\\vdots\\\\\n",
    "\\vec{x_n}\n",
    "\\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Thus, we can write $$(w^{(t)})^Tx_j = \\begin{pmatrix}\n",
    "a_1 & a_2 & ... & a_n\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "\\vec{x_1}\\\\\n",
    "\\vec{x_2}\\\\\n",
    "\\vdots\\\\\n",
    "\\vec{x_n}\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "x_{{j}_{1}}\\\\\n",
    "x_{{j}_{2}}\\\\\n",
    "\\vdots\\\\\n",
    "x_{{j}_{d}}\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "a_1 & a_2 & ... & a_n\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "k(x_1,x_j)\\\\\n",
    "k(x_2,x_j)\\\\\n",
    "\\vdots\\\\\n",
    "k(x_n,x_j)\n",
    "\\end{pmatrix} = (a^{t})^T(K_{j.})^T = K_{j.}a^{(t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no margin violation, $w^{(t+1)} = (1-\\eta_t\\lambda)w^{(t)}$. Thus, $$a^{(t+1)} =(1-\\eta_t\\lambda) a^{(t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, write $w^{(t)}$ as $w^{(t)} = \\begin{pmatrix}\n",
    "\\vec{x_1} & ... & \\vec{x_n}\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "a_1 \\\\\n",
    "\\vdots \\\\\n",
    "a_n\n",
    "\\end{pmatrix} = X^Ta^{(t)}$, and let $\\vec{1_j} = \\begin{pmatrix}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{pmatrix}$ be the $n$-dimensional row vector with only the $j$-th entry being $1$, rest is all $0$.                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, given the update on with margin violation example $x_j$: $$w^{(t+1)} = (1-\\eta_t\\lambda)w^{(t)} + \\eta_t y_jx_j,$$ we can rewrite it as $$X^Ta^{(t+1)} = (1-\\eta_t\\lambda)X^Ta^{(t)} + \\eta_t y_j X^T \\vec{1_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing $X^T$, we get $$a^{(t+1)} = (1-\\eta_t\\lambda)a^{(t)} + \\eta_t y_j  \\vec{1_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"screenshot.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsga-1003]",
   "language": "python",
   "name": "conda-env-dsga-1003-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
